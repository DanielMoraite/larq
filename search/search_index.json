{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Larq \u00b6 Larq is an open source machine learning library for training Quantized Neural Networks (QNNs) with extremely low precision weights and activations (e.g. 1-bit). Existing Deep Neural Networks tend to be large, slow and power-hungry, prohibiting many applications in resource-constrained environments. Larq is designed to provide an easy to use, composable way to train QNNs (e.g. Binarized Neural Networks) based on the tf.keras interface. Getting Started \u00b6 To build a QNN Larq introduces the concept of Quantized Layers and Quantizers . A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a binarized densely-connected layer using the Straight-Through Estimator the following way: larq . layers . QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) This layer can be used inside a keras model or with a custom training loop . Examples \u00b6 Checkout our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to Larq Binarynet on CIFAR10 Requirements \u00b6 Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also checkout one of our prebuilt docker images . Installation \u00b6 You can install Larq with Python's pip package manager: pip install larq","title":"Getting Started"},{"location":"#larq","text":"Larq is an open source machine learning library for training Quantized Neural Networks (QNNs) with extremely low precision weights and activations (e.g. 1-bit). Existing Deep Neural Networks tend to be large, slow and power-hungry, prohibiting many applications in resource-constrained environments. Larq is designed to provide an easy to use, composable way to train QNNs (e.g. Binarized Neural Networks) based on the tf.keras interface.","title":"Larq"},{"location":"#getting-started","text":"To build a QNN Larq introduces the concept of Quantized Layers and Quantizers . A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a binarized densely-connected layer using the Straight-Through Estimator the following way: larq . layers . QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) This layer can be used inside a keras model or with a custom training loop .","title":"Getting Started"},{"location":"#examples","text":"Checkout our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to Larq Binarynet on CIFAR10","title":"Examples"},{"location":"#requirements","text":"Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also checkout one of our prebuilt docker images .","title":"Requirements"},{"location":"#installation","text":"You can install Larq with Python's pip package manager: pip install larq","title":"Installation"},{"location":"activations/","text":"larq.activations \u00b6 Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh )) hard_tanh \u00b6 hard_tanh ( x ) Hard tanh activation function. \\[\\sigma(x) = \\mathrm{Clip}(x, \u22121, 1)\\] Arguments x : Input tensor. Returns Hard tanh activation.","title":"Activations"},{"location":"activations/#larqactivations","text":"Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh ))","title":"larq.activations"},{"location":"activations/#hard_tanh","text":"hard_tanh ( x ) Hard tanh activation function. \\[\\sigma(x) = \\mathrm{Clip}(x, \u22121, 1)\\] Arguments x : Input tensor. Returns Hard tanh activation.","title":"hard_tanh"},{"location":"callbacks/","text":"larq.callbacks \u00b6 QuantizationLogger \u00b6 QuantizationLogger ( update_freq = 'batch' ) Callback that adds quantization specific metrics. In order for metrics to be picked up by TensorBoard this callback needs to be applied before the TensorBoard callback and use the same update frequency. Example callbacks = [ QuantizationLogger ( update_freq = 100 ), tf . keras . callbacks . TensorBoard ( update_freq = 100 ), ] model . fit ( X_train , Y_train , callbacks = callbacks ) Metrics changed_quantization_ration : The ration of quantized weights in each layer that changed during the weight update. Arguments update_freq : 'batch' or integer. When using 'batch' , computes the metrics after each batch. If using an integer the callback will compute the metrics every update_freq batches. Note that computing too frequently can slow down training.","title":"Callbacks"},{"location":"callbacks/#larqcallbacks","text":"","title":"larq.callbacks"},{"location":"callbacks/#quantizationlogger","text":"QuantizationLogger ( update_freq = 'batch' ) Callback that adds quantization specific metrics. In order for metrics to be picked up by TensorBoard this callback needs to be applied before the TensorBoard callback and use the same update frequency. Example callbacks = [ QuantizationLogger ( update_freq = 100 ), tf . keras . callbacks . TensorBoard ( update_freq = 100 ), ] model . fit ( X_train , Y_train , callbacks = callbacks ) Metrics changed_quantization_ration : The ration of quantized weights in each layer that changed during the weight update. Arguments update_freq : 'batch' or integer. When using 'batch' , computes the metrics after each batch. If using an integer the callback will compute the metrics every update_freq batches. Note that computing too frequently can slow down training.","title":"QuantizationLogger"},{"location":"constraints/","text":"larq.constraints \u00b6 Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. )) WeightClip \u00b6 WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights.","title":"Constraints"},{"location":"constraints/#larqconstraints","text":"Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. ))","title":"larq.constraints"},{"location":"constraints/#weightclip","text":"WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights.","title":"WeightClip"},{"location":"layers/","text":"larq.layers \u00b6 Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. QuantDense \u00b6 QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) . QuantConv1D \u00b6 QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides. QuantConv2D \u00b6 QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv3D \u00b6 QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding. QuantSeparableConv1D \u00b6 QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer. QuantSeparableConv2D \u00b6 QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv2DTranspose \u00b6 QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantConv3DTranspose \u00b6 QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantLocallyConnected1D \u00b6 QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides. QuantLocallyConnected2D \u00b6 QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"Quantized Layers"},{"location":"layers/#larqlayers","text":"Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer.","title":"larq.layers"},{"location":"layers/#quantdense","text":"QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) .","title":"QuantDense"},{"location":"layers/#quantconv1d","text":"QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides.","title":"QuantConv1D"},{"location":"layers/#quantconv2d","text":"QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantConv2D"},{"location":"layers/#quantconv3d","text":"QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding.","title":"QuantConv3D"},{"location":"layers/#quantseparableconv1d","text":"QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer.","title":"QuantSeparableConv1D"},{"location":"layers/#quantseparableconv2d","text":"QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantSeparableConv2D"},{"location":"layers/#quantconv2dtranspose","text":"QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv2DTranspose"},{"location":"layers/#quantconv3dtranspose","text":"QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv3DTranspose"},{"location":"layers/#quantlocallyconnected1d","text":"QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides.","title":"QuantLocallyConnected1D"},{"location":"layers/#quantlocallyconnected2d","text":"QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantLocallyConnected2D"},{"location":"models/","text":"larq.models \u00b6 summary \u00b6 summary ( model , tablefmt = 'simple' , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. tablefmt : Supported table formats are: fancy_grid , github , grid , html , jira , latex , latex_booktabs , latex_raw , mediawiki , moinmoin , orgtbl , pipe , plain , presto , psql , rst , simple , textile , tsv , youtrac . print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"Models"},{"location":"models/#larqmodels","text":"","title":"larq.models"},{"location":"models/#summary","text":"summary ( model , tablefmt = 'simple' , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. tablefmt : Supported table formats are: fancy_grid , github , grid , html , jira , latex , latex_booktabs , latex_raw , mediawiki , moinmoin , orgtbl , pipe , plain , presto , psql , rst , simple , textile , tsv , youtrac . print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"summary"},{"location":"optimizers/","text":"larq.optimizers \u00b6 XavierLearningRateScaling \u00b6 XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"Optimizers"},{"location":"optimizers/#larqoptimizers","text":"","title":"larq.optimizers"},{"location":"optimizers/#xavierlearningratescaling","text":"XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"XavierLearningRateScaling"},{"location":"quantizers/","text":"larq.quantizers \u00b6 A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. ste_sign \u00b6 ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 approx_sign \u00b6 approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"Quantizers"},{"location":"quantizers/#larqquantizers","text":"A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass.","title":"larq.quantizers"},{"location":"quantizers/#ste_sign","text":"ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1","title":"ste_sign"},{"location":"quantizers/#approx_sign","text":"approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"approx_sign"},{"location":"dev/code_of_conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"dev/code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"dev/code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"dev/code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"dev/code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"dev/code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"dev/code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"dev/code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"dev/contributing/","text":"Contributing to Larq \u00b6 \ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub . Project setup \u00b6 To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies Run Unit tests \u00b6 Inside the project directory run: pytest . Build documentation \u00b6 Inside the project directory run: pip install -e . [ docs ] # Installs dependencies for building the docs pydocmd serve Code style \u00b6 We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor .","title":"Contributing Guide"},{"location":"dev/contributing/#contributing-to-larq","text":"\ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub .","title":"Contributing to Larq"},{"location":"dev/contributing/#project-setup","text":"To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies","title":"Project setup"},{"location":"dev/contributing/#run-unit-tests","text":"Inside the project directory run: pytest .","title":"Run Unit tests"},{"location":"dev/contributing/#build-documentation","text":"Inside the project directory run: pip install -e . [ docs ] # Installs dependencies for building the docs pydocmd serve","title":"Build documentation"},{"location":"dev/contributing/#code-style","text":"We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor .","title":"Code style"},{"location":"examples/binarynet_cifar10/","text":"Binarynet on CIFAR10 \u00b6 In this example we demonstrate how to use Larq to build binarynet for CIFAR10 to achieve a validation accuracy around 83% on laptop hardware. On a Nvidia GTX1050ti MaxQ it takes approximately 200 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening. Import Modules \u00b6 First we import the modules. We use tensorflow, Keras and Larq. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt Import CIFAR10 Dataset \u00b6 We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) train_images = train_images . astype ( 'float32' ) test_images = test_images . astype ( 'float32' ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1. , test_images / 127.5 - 1. train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes ) Build Binarynet \u00b6 Here we build the binarynet model layer by layer using a keras sequential model model = tf . keras . models . Sequential ( [ lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , use_bias = False , kernel_constraint = \"weight_clip\" , input_shape = ( 32 , 32 , 3 ), ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 256 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 256 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 512 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 512 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 10 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ), ] ) One can output a summary of the model. lq . models . summary ( model ) Layer Outputs # 1-bit # 32-bit ------------------------ ----------------- --------- ---------- quant_conv2d (-1, 30, 30, 128) 3456 0 batch_normalization_v1 (-1, 30, 30, 128) 0 384 activation (-1, 30, 30, 128) 0 0 quant_conv2d_1 (-1, 30, 30, 128) 147456 0 max_pooling2d (-1, 15, 15, 128) 0 0 batch_normalization_v1_1 (-1, 15, 15, 128) 0 384 activation_1 (-1, 15, 15, 128) 0 0 quant_conv2d_2 (-1, 15, 15, 256) 294912 0 batch_normalization_v1_2 (-1, 15, 15, 256) 0 768 activation_2 (-1, 15, 15, 256) 0 0 quant_conv2d_3 (-1, 15, 15, 256) 589824 0 max_pooling2d_1 (-1, 7, 7, 256) 0 0 batch_normalization_v1_3 (-1, 7, 7, 256) 0 768 activation_3 (-1, 7, 7, 256) 0 0 quant_conv2d_4 (-1, 7, 7, 512) 1179648 0 batch_normalization_v1_4 (-1, 7, 7, 512) 0 1536 activation_4 (-1, 7, 7, 512) 0 0 quant_conv2d_5 (-1, 7, 7, 512) 2359296 0 max_pooling2d_2 (-1, 3, 3, 512) 0 0 batch_normalization_v1_5 (-1, 3, 3, 512) 0 1536 flatten (-1, 4608) 0 0 activation_5 (-1, 4608) 0 0 quant_dense (-1, 1024) 4718592 0 batch_normalization_v1_6 (-1, 1024) 0 3072 activation_6 (-1, 1024) 0 0 quant_dense_1 (-1, 1024) 1048576 0 batch_normalization_v1_7 (-1, 1024) 0 3072 activation_7 (-1, 1024) 0 0 quant_dense_2 (-1, 10) 10240 0 batch_normalization_v1_8 (-1, 10) 0 30 activation_8 (-1, 10) 0 0 Total 10352000 11550 Total params: 10363550 Trainable params: 10355850 Non-trainable params: 7700 Model Training \u00b6 Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples, validate on 10000 samples Epoch 1/100 50000/50000 [==============================] - 131s 3ms/step - loss: 1.5733 - acc: 0.4533 - val_loss: 1.6368 - val_acc: 0.4244 Epoch 2/100 50000/50000 [==============================] - 125s 3ms/step - loss: 1.1485 - acc: 0.6387 - val_loss: 1.8497 - val_acc: 0.3764 Epoch 3/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.9641 - acc: 0.7207 - val_loss: 1.5696 - val_acc: 0.4794 Epoch 4/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.8452 - acc: 0.7728 - val_loss: 1.5765 - val_acc: 0.4669 Epoch 5/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.7553 - acc: 0.8114 - val_loss: 1.0653 - val_acc: 0.6928 Epoch 6/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.6841 - acc: 0.8447 - val_loss: 1.0944 - val_acc: 0.6880 Epoch 7/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.6356 - acc: 0.8685 - val_loss: 0.9909 - val_acc: 0.7317 Epoch 8/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.5907 - acc: 0.8910 - val_loss: 0.9453 - val_acc: 0.7446 Epoch 9/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.5610 - acc: 0.9043 - val_loss: 0.9441 - val_acc: 0.7460 Epoch 10/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.5295 - acc: 0.9201 - val_loss: 0.8892 - val_acc: 0.7679 Epoch 11/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.5100 - acc: 0.9309 - val_loss: 0.8808 - val_acc: 0.7818 Epoch 12/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4926 - acc: 0.9397 - val_loss: 0.8404 - val_acc: 0.7894 Epoch 13/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4807 - acc: 0.9470 - val_loss: 0.8600 - val_acc: 0.7928 Epoch 14/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4661 - acc: 0.9529 - val_loss: 0.9046 - val_acc: 0.7732 Epoch 15/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.4588 - acc: 0.9571 - val_loss: 0.8505 - val_acc: 0.7965 Epoch 16/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4558 - acc: 0.9593 - val_loss: 0.8748 - val_acc: 0.7859 Epoch 17/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4434 - acc: 0.9649 - val_loss: 0.9109 - val_acc: 0.7656 Epoch 18/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4449 - acc: 0.9643 - val_loss: 0.8532 - val_acc: 0.7971 Epoch 19/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4349 - acc: 0.9701 - val_loss: 0.8677 - val_acc: 0.7951 Epoch 20/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4351 - acc: 0.9698 - val_loss: 0.9145 - val_acc: 0.7740 Epoch 21/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4268 - acc: 0.9740 - val_loss: 0.8308 - val_acc: 0.8065 Epoch 22/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4243 - acc: 0.9741 - val_loss: 0.8229 - val_acc: 0.8075 Epoch 23/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4201 - acc: 0.9764 - val_loss: 0.8411 - val_acc: 0.8062 Epoch 24/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4190 - acc: 0.9769 - val_loss: 0.8649 - val_acc: 0.7951 Epoch 25/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4139 - acc: 0.9787 - val_loss: 0.8257 - val_acc: 0.8071 Epoch 26/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4154 - acc: 0.9779 - val_loss: 0.8041 - val_acc: 0.8205 Epoch 27/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4128 - acc: 0.9798 - val_loss: 0.8296 - val_acc: 0.8115 Epoch 28/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4121 - acc: 0.9798 - val_loss: 0.8241 - val_acc: 0.8074 Epoch 29/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4093 - acc: 0.9807 - val_loss: 0.8575 - val_acc: 0.7913 Epoch 30/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4048 - acc: 0.9826 - val_loss: 0.8118 - val_acc: 0.8166 Epoch 31/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4041 - acc: 0.9837 - val_loss: 0.8375 - val_acc: 0.8082 Epoch 32/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4045 - acc: 0.9831 - val_loss: 0.8604 - val_acc: 0.8091 Epoch 33/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4047 - acc: 0.9823 - val_loss: 0.8797 - val_acc: 0.7931 Epoch 34/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4023 - acc: 0.9842 - val_loss: 0.8694 - val_acc: 0.8020 Epoch 35/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.3995 - acc: 0.9858 - val_loss: 0.8161 - val_acc: 0.8186 Epoch 36/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3976 - acc: 0.9859 - val_loss: 0.8495 - val_acc: 0.7988 Epoch 37/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4021 - acc: 0.9847 - val_loss: 0.8542 - val_acc: 0.8062 Epoch 38/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3939 - acc: 0.9869 - val_loss: 0.8347 - val_acc: 0.8122 Epoch 39/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3955 - acc: 0.9856 - val_loss: 0.8521 - val_acc: 0.7993 Epoch 40/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3907 - acc: 0.9885 - val_loss: 0.9023 - val_acc: 0.7992 Epoch 41/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3911 - acc: 0.9873 - val_loss: 0.8597 - val_acc: 0.8010 Epoch 42/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3917 - acc: 0.9885 - val_loss: 0.8968 - val_acc: 0.7936 Epoch 43/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3931 - acc: 0.9874 - val_loss: 0.8318 - val_acc: 0.8169 Epoch 44/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3897 - acc: 0.9893 - val_loss: 0.8811 - val_acc: 0.7988 Epoch 45/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3876 - acc: 0.9888 - val_loss: 0.8453 - val_acc: 0.8094 Epoch 46/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3876 - acc: 0.9889 - val_loss: 0.8195 - val_acc: 0.8179 Epoch 47/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3891 - acc: 0.9890 - val_loss: 0.8373 - val_acc: 0.8137 Epoch 48/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3902 - acc: 0.9888 - val_loss: 0.8457 - val_acc: 0.8120 Epoch 49/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3864 - acc: 0.9903 - val_loss: 0.9012 - val_acc: 0.7907 Epoch 50/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3859 - acc: 0.9903 - val_loss: 0.8291 - val_acc: 0.8053 Epoch 51/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3830 - acc: 0.9915 - val_loss: 0.8494 - val_acc: 0.8139 Epoch 52/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3828 - acc: 0.9907 - val_loss: 0.8447 - val_acc: 0.8135 Epoch 53/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3823 - acc: 0.9910 - val_loss: 0.8539 - val_acc: 0.8120 Epoch 54/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3832 - acc: 0.9905 - val_loss: 0.8592 - val_acc: 0.8098 Epoch 55/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3823 - acc: 0.9908 - val_loss: 0.8585 - val_acc: 0.8087 Epoch 56/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3817 - acc: 0.9911 - val_loss: 0.8840 - val_acc: 0.7889 Epoch 57/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3827 - acc: 0.9914 - val_loss: 0.8205 - val_acc: 0.8250 Epoch 58/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3818 - acc: 0.9912 - val_loss: 0.8571 - val_acc: 0.8051 Epoch 59/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3811 - acc: 0.9919 - val_loss: 0.8155 - val_acc: 0.8254 Epoch 60/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.3803 - acc: 0.9919 - val_loss: 0.8617 - val_acc: 0.8040 Epoch 61/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3793 - acc: 0.9926 - val_loss: 0.8212 - val_acc: 0.8192 Epoch 62/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3825 - acc: 0.9912 - val_loss: 0.8139 - val_acc: 0.8277 Epoch 63/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3784 - acc: 0.9923 - val_loss: 0.8304 - val_acc: 0.8121 Epoch 64/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3809 - acc: 0.9918 - val_loss: 0.7961 - val_acc: 0.8289 Epoch 65/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3750 - acc: 0.9930 - val_loss: 0.8676 - val_acc: 0.8110 Epoch 66/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3789 - acc: 0.9928 - val_loss: 0.8308 - val_acc: 0.8148 Epoch 67/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3783 - acc: 0.9929 - val_loss: 0.8595 - val_acc: 0.8097 Epoch 68/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3758 - acc: 0.9935 - val_loss: 0.8359 - val_acc: 0.8065 Epoch 69/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3784 - acc: 0.9927 - val_loss: 0.8189 - val_acc: 0.8255 Epoch 70/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3786 - acc: 0.9924 - val_loss: 0.8754 - val_acc: 0.8001 Epoch 71/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3749 - acc: 0.9936 - val_loss: 0.8188 - val_acc: 0.8262 Epoch 72/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3758 - acc: 0.9932 - val_loss: 0.8540 - val_acc: 0.8169 Epoch 73/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3740 - acc: 0.9934 - val_loss: 0.8127 - val_acc: 0.8258 Epoch 74/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3749 - acc: 0.9932 - val_loss: 0.8662 - val_acc: 0.8018 Epoch 75/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3721 - acc: 0.9941 - val_loss: 0.8359 - val_acc: 0.8213 Epoch 76/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3746 - acc: 0.9937 - val_loss: 0.8462 - val_acc: 0.8178 Epoch 77/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3741 - acc: 0.9936 - val_loss: 0.8983 - val_acc: 0.7972 Epoch 78/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3751 - acc: 0.9933 - val_loss: 0.8525 - val_acc: 0.8173 Epoch 79/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3762 - acc: 0.9931 - val_loss: 0.8190 - val_acc: 0.8201 Epoch 80/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3737 - acc: 0.9940 - val_loss: 0.8441 - val_acc: 0.8196 Epoch 81/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3729 - acc: 0.9935 - val_loss: 0.8151 - val_acc: 0.8267 Epoch 82/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3735 - acc: 0.9938 - val_loss: 0.8405 - val_acc: 0.8163 Epoch 83/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3723 - acc: 0.9939 - val_loss: 0.8225 - val_acc: 0.8243 Epoch 84/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3738 - acc: 0.9938 - val_loss: 0.8413 - val_acc: 0.8115 Epoch 85/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3714 - acc: 0.9947 - val_loss: 0.9080 - val_acc: 0.7932 Epoch 86/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3744 - acc: 0.9942 - val_loss: 0.8467 - val_acc: 0.8135 Epoch 87/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3705 - acc: 0.9948 - val_loss: 0.8491 - val_acc: 0.8163 Epoch 88/100 50000/50000 [==============================] - 128s 3ms/step - loss: 0.3733 - acc: 0.9944 - val_loss: 0.8005 - val_acc: 0.8214 Epoch 89/100 50000/50000 [==============================] - 134s 3ms/step - loss: 0.3693 - acc: 0.9949 - val_loss: 0.7791 - val_acc: 0.8321 Epoch 90/100 50000/50000 [==============================] - 135s 3ms/step - loss: 0.3724 - acc: 0.9942 - val_loss: 0.8458 - val_acc: 0.8124 Epoch 91/100 50000/50000 [==============================] - 128s 3ms/step - loss: 0.3732 - acc: 0.9947 - val_loss: 0.8315 - val_acc: 0.8164 Epoch 92/100 50000/50000 [==============================] - 127s 3ms/step - loss: 0.3699 - acc: 0.9950 - val_loss: 0.8140 - val_acc: 0.8226 Epoch 93/100 50000/50000 [==============================] - 131s 3ms/step - loss: 0.3694 - acc: 0.9950 - val_loss: 0.8342 - val_acc: 0.8210 Epoch 94/100 50000/50000 [==============================] - 134s 3ms/step - loss: 0.3698 - acc: 0.9946 - val_loss: 0.8938 - val_acc: 0.8019 Epoch 95/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3698 - acc: 0.9946 - val_loss: 0.8771 - val_acc: 0.8066 Epoch 96/100 50000/50000 [==============================] - 164s 3ms/step - loss: 0.3712 - acc: 0.9946 - val_loss: 0.8396 - val_acc: 0.8211 Epoch 97/100 50000/50000 [==============================] - 155s 3ms/step - loss: 0.3689 - acc: 0.9949 - val_loss: 0.8728 - val_acc: 0.8112 Epoch 98/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3663 - acc: 0.9953 - val_loss: 0.9615 - val_acc: 0.7902 Epoch 99/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3714 - acc: 0.9944 - val_loss: 0.8414 - val_acc: 0.8188 Epoch 100/100 50000/50000 [==============================] - 138s 3ms/step - loss: 0.3682 - acc: 0.9956 - val_loss: 0.8055 - val_acc: 0.8266 Model Output \u00b6 We can now plot a the final validation accuracy and loss plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0.9956000019311905 0.8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0.3663262344896793 0.7790719392895699","title":"Binarynet on CIFAR10"},{"location":"examples/binarynet_cifar10/#binarynet-on-cifar10","text":"In this example we demonstrate how to use Larq to build binarynet for CIFAR10 to achieve a validation accuracy around 83% on laptop hardware. On a Nvidia GTX1050ti MaxQ it takes approximately 200 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening.","title":"Binarynet on CIFAR10"},{"location":"examples/binarynet_cifar10/#import-modules","text":"First we import the modules. We use tensorflow, Keras and Larq. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt","title":"Import Modules"},{"location":"examples/binarynet_cifar10/#import-cifar10-dataset","text":"We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) train_images = train_images . astype ( 'float32' ) test_images = test_images . astype ( 'float32' ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1. , test_images / 127.5 - 1. train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes )","title":"Import CIFAR10 Dataset"},{"location":"examples/binarynet_cifar10/#build-binarynet","text":"Here we build the binarynet model layer by layer using a keras sequential model model = tf . keras . models . Sequential ( [ lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , use_bias = False , kernel_constraint = \"weight_clip\" , input_shape = ( 32 , 32 , 3 ), ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 256 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 256 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 512 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantConv2D ( 512 , 3 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , padding = \"same\" , use_bias = False , ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"hard_tanh\" ), lq . layers . QuantDense ( 10 , kernel_quantizer = \"ste_sign\" , input_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ), ] ) One can output a summary of the model. lq . models . summary ( model ) Layer Outputs # 1-bit # 32-bit ------------------------ ----------------- --------- ---------- quant_conv2d (-1, 30, 30, 128) 3456 0 batch_normalization_v1 (-1, 30, 30, 128) 0 384 activation (-1, 30, 30, 128) 0 0 quant_conv2d_1 (-1, 30, 30, 128) 147456 0 max_pooling2d (-1, 15, 15, 128) 0 0 batch_normalization_v1_1 (-1, 15, 15, 128) 0 384 activation_1 (-1, 15, 15, 128) 0 0 quant_conv2d_2 (-1, 15, 15, 256) 294912 0 batch_normalization_v1_2 (-1, 15, 15, 256) 0 768 activation_2 (-1, 15, 15, 256) 0 0 quant_conv2d_3 (-1, 15, 15, 256) 589824 0 max_pooling2d_1 (-1, 7, 7, 256) 0 0 batch_normalization_v1_3 (-1, 7, 7, 256) 0 768 activation_3 (-1, 7, 7, 256) 0 0 quant_conv2d_4 (-1, 7, 7, 512) 1179648 0 batch_normalization_v1_4 (-1, 7, 7, 512) 0 1536 activation_4 (-1, 7, 7, 512) 0 0 quant_conv2d_5 (-1, 7, 7, 512) 2359296 0 max_pooling2d_2 (-1, 3, 3, 512) 0 0 batch_normalization_v1_5 (-1, 3, 3, 512) 0 1536 flatten (-1, 4608) 0 0 activation_5 (-1, 4608) 0 0 quant_dense (-1, 1024) 4718592 0 batch_normalization_v1_6 (-1, 1024) 0 3072 activation_6 (-1, 1024) 0 0 quant_dense_1 (-1, 1024) 1048576 0 batch_normalization_v1_7 (-1, 1024) 0 3072 activation_7 (-1, 1024) 0 0 quant_dense_2 (-1, 10) 10240 0 batch_normalization_v1_8 (-1, 10) 0 30 activation_8 (-1, 10) 0 0 Total 10352000 11550 Total params: 10363550 Trainable params: 10355850 Non-trainable params: 7700","title":"Build Binarynet"},{"location":"examples/binarynet_cifar10/#model-training","text":"Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples, validate on 10000 samples Epoch 1/100 50000/50000 [==============================] - 131s 3ms/step - loss: 1.5733 - acc: 0.4533 - val_loss: 1.6368 - val_acc: 0.4244 Epoch 2/100 50000/50000 [==============================] - 125s 3ms/step - loss: 1.1485 - acc: 0.6387 - val_loss: 1.8497 - val_acc: 0.3764 Epoch 3/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.9641 - acc: 0.7207 - val_loss: 1.5696 - val_acc: 0.4794 Epoch 4/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.8452 - acc: 0.7728 - val_loss: 1.5765 - val_acc: 0.4669 Epoch 5/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.7553 - acc: 0.8114 - val_loss: 1.0653 - val_acc: 0.6928 Epoch 6/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.6841 - acc: 0.8447 - val_loss: 1.0944 - val_acc: 0.6880 Epoch 7/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.6356 - acc: 0.8685 - val_loss: 0.9909 - val_acc: 0.7317 Epoch 8/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.5907 - acc: 0.8910 - val_loss: 0.9453 - val_acc: 0.7446 Epoch 9/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.5610 - acc: 0.9043 - val_loss: 0.9441 - val_acc: 0.7460 Epoch 10/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.5295 - acc: 0.9201 - val_loss: 0.8892 - val_acc: 0.7679 Epoch 11/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.5100 - acc: 0.9309 - val_loss: 0.8808 - val_acc: 0.7818 Epoch 12/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4926 - acc: 0.9397 - val_loss: 0.8404 - val_acc: 0.7894 Epoch 13/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4807 - acc: 0.9470 - val_loss: 0.8600 - val_acc: 0.7928 Epoch 14/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4661 - acc: 0.9529 - val_loss: 0.9046 - val_acc: 0.7732 Epoch 15/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.4588 - acc: 0.9571 - val_loss: 0.8505 - val_acc: 0.7965 Epoch 16/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4558 - acc: 0.9593 - val_loss: 0.8748 - val_acc: 0.7859 Epoch 17/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4434 - acc: 0.9649 - val_loss: 0.9109 - val_acc: 0.7656 Epoch 18/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4449 - acc: 0.9643 - val_loss: 0.8532 - val_acc: 0.7971 Epoch 19/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4349 - acc: 0.9701 - val_loss: 0.8677 - val_acc: 0.7951 Epoch 20/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4351 - acc: 0.9698 - val_loss: 0.9145 - val_acc: 0.7740 Epoch 21/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4268 - acc: 0.9740 - val_loss: 0.8308 - val_acc: 0.8065 Epoch 22/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4243 - acc: 0.9741 - val_loss: 0.8229 - val_acc: 0.8075 Epoch 23/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4201 - acc: 0.9764 - val_loss: 0.8411 - val_acc: 0.8062 Epoch 24/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4190 - acc: 0.9769 - val_loss: 0.8649 - val_acc: 0.7951 Epoch 25/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4139 - acc: 0.9787 - val_loss: 0.8257 - val_acc: 0.8071 Epoch 26/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4154 - acc: 0.9779 - val_loss: 0.8041 - val_acc: 0.8205 Epoch 27/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4128 - acc: 0.9798 - val_loss: 0.8296 - val_acc: 0.8115 Epoch 28/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4121 - acc: 0.9798 - val_loss: 0.8241 - val_acc: 0.8074 Epoch 29/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4093 - acc: 0.9807 - val_loss: 0.8575 - val_acc: 0.7913 Epoch 30/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4048 - acc: 0.9826 - val_loss: 0.8118 - val_acc: 0.8166 Epoch 31/100 50000/50000 [==============================] - 126s 3ms/step - loss: 0.4041 - acc: 0.9837 - val_loss: 0.8375 - val_acc: 0.8082 Epoch 32/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.4045 - acc: 0.9831 - val_loss: 0.8604 - val_acc: 0.8091 Epoch 33/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4047 - acc: 0.9823 - val_loss: 0.8797 - val_acc: 0.7931 Epoch 34/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.4023 - acc: 0.9842 - val_loss: 0.8694 - val_acc: 0.8020 Epoch 35/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.3995 - acc: 0.9858 - val_loss: 0.8161 - val_acc: 0.8186 Epoch 36/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3976 - acc: 0.9859 - val_loss: 0.8495 - val_acc: 0.7988 Epoch 37/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.4021 - acc: 0.9847 - val_loss: 0.8542 - val_acc: 0.8062 Epoch 38/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3939 - acc: 0.9869 - val_loss: 0.8347 - val_acc: 0.8122 Epoch 39/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3955 - acc: 0.9856 - val_loss: 0.8521 - val_acc: 0.7993 Epoch 40/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3907 - acc: 0.9885 - val_loss: 0.9023 - val_acc: 0.7992 Epoch 41/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3911 - acc: 0.9873 - val_loss: 0.8597 - val_acc: 0.8010 Epoch 42/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3917 - acc: 0.9885 - val_loss: 0.8968 - val_acc: 0.7936 Epoch 43/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3931 - acc: 0.9874 - val_loss: 0.8318 - val_acc: 0.8169 Epoch 44/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3897 - acc: 0.9893 - val_loss: 0.8811 - val_acc: 0.7988 Epoch 45/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3876 - acc: 0.9888 - val_loss: 0.8453 - val_acc: 0.8094 Epoch 46/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3876 - acc: 0.9889 - val_loss: 0.8195 - val_acc: 0.8179 Epoch 47/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3891 - acc: 0.9890 - val_loss: 0.8373 - val_acc: 0.8137 Epoch 48/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3902 - acc: 0.9888 - val_loss: 0.8457 - val_acc: 0.8120 Epoch 49/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3864 - acc: 0.9903 - val_loss: 0.9012 - val_acc: 0.7907 Epoch 50/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3859 - acc: 0.9903 - val_loss: 0.8291 - val_acc: 0.8053 Epoch 51/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3830 - acc: 0.9915 - val_loss: 0.8494 - val_acc: 0.8139 Epoch 52/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3828 - acc: 0.9907 - val_loss: 0.8447 - val_acc: 0.8135 Epoch 53/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3823 - acc: 0.9910 - val_loss: 0.8539 - val_acc: 0.8120 Epoch 54/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3832 - acc: 0.9905 - val_loss: 0.8592 - val_acc: 0.8098 Epoch 55/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3823 - acc: 0.9908 - val_loss: 0.8585 - val_acc: 0.8087 Epoch 56/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3817 - acc: 0.9911 - val_loss: 0.8840 - val_acc: 0.7889 Epoch 57/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3827 - acc: 0.9914 - val_loss: 0.8205 - val_acc: 0.8250 Epoch 58/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3818 - acc: 0.9912 - val_loss: 0.8571 - val_acc: 0.8051 Epoch 59/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3811 - acc: 0.9919 - val_loss: 0.8155 - val_acc: 0.8254 Epoch 60/100 50000/50000 [==============================] - 125s 3ms/step - loss: 0.3803 - acc: 0.9919 - val_loss: 0.8617 - val_acc: 0.8040 Epoch 61/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3793 - acc: 0.9926 - val_loss: 0.8212 - val_acc: 0.8192 Epoch 62/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3825 - acc: 0.9912 - val_loss: 0.8139 - val_acc: 0.8277 Epoch 63/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3784 - acc: 0.9923 - val_loss: 0.8304 - val_acc: 0.8121 Epoch 64/100 50000/50000 [==============================] - 125s 2ms/step - loss: 0.3809 - acc: 0.9918 - val_loss: 0.7961 - val_acc: 0.8289 Epoch 65/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3750 - acc: 0.9930 - val_loss: 0.8676 - val_acc: 0.8110 Epoch 66/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3789 - acc: 0.9928 - val_loss: 0.8308 - val_acc: 0.8148 Epoch 67/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3783 - acc: 0.9929 - val_loss: 0.8595 - val_acc: 0.8097 Epoch 68/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3758 - acc: 0.9935 - val_loss: 0.8359 - val_acc: 0.8065 Epoch 69/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3784 - acc: 0.9927 - val_loss: 0.8189 - val_acc: 0.8255 Epoch 70/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3786 - acc: 0.9924 - val_loss: 0.8754 - val_acc: 0.8001 Epoch 71/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3749 - acc: 0.9936 - val_loss: 0.8188 - val_acc: 0.8262 Epoch 72/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3758 - acc: 0.9932 - val_loss: 0.8540 - val_acc: 0.8169 Epoch 73/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3740 - acc: 0.9934 - val_loss: 0.8127 - val_acc: 0.8258 Epoch 74/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3749 - acc: 0.9932 - val_loss: 0.8662 - val_acc: 0.8018 Epoch 75/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3721 - acc: 0.9941 - val_loss: 0.8359 - val_acc: 0.8213 Epoch 76/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3746 - acc: 0.9937 - val_loss: 0.8462 - val_acc: 0.8178 Epoch 77/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3741 - acc: 0.9936 - val_loss: 0.8983 - val_acc: 0.7972 Epoch 78/100 50000/50000 [==============================] - 122s 2ms/step - loss: 0.3751 - acc: 0.9933 - val_loss: 0.8525 - val_acc: 0.8173 Epoch 79/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3762 - acc: 0.9931 - val_loss: 0.8190 - val_acc: 0.8201 Epoch 80/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3737 - acc: 0.9940 - val_loss: 0.8441 - val_acc: 0.8196 Epoch 81/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3729 - acc: 0.9935 - val_loss: 0.8151 - val_acc: 0.8267 Epoch 82/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3735 - acc: 0.9938 - val_loss: 0.8405 - val_acc: 0.8163 Epoch 83/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3723 - acc: 0.9939 - val_loss: 0.8225 - val_acc: 0.8243 Epoch 84/100 50000/50000 [==============================] - 123s 2ms/step - loss: 0.3738 - acc: 0.9938 - val_loss: 0.8413 - val_acc: 0.8115 Epoch 85/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3714 - acc: 0.9947 - val_loss: 0.9080 - val_acc: 0.7932 Epoch 86/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3744 - acc: 0.9942 - val_loss: 0.8467 - val_acc: 0.8135 Epoch 87/100 50000/50000 [==============================] - 124s 2ms/step - loss: 0.3705 - acc: 0.9948 - val_loss: 0.8491 - val_acc: 0.8163 Epoch 88/100 50000/50000 [==============================] - 128s 3ms/step - loss: 0.3733 - acc: 0.9944 - val_loss: 0.8005 - val_acc: 0.8214 Epoch 89/100 50000/50000 [==============================] - 134s 3ms/step - loss: 0.3693 - acc: 0.9949 - val_loss: 0.7791 - val_acc: 0.8321 Epoch 90/100 50000/50000 [==============================] - 135s 3ms/step - loss: 0.3724 - acc: 0.9942 - val_loss: 0.8458 - val_acc: 0.8124 Epoch 91/100 50000/50000 [==============================] - 128s 3ms/step - loss: 0.3732 - acc: 0.9947 - val_loss: 0.8315 - val_acc: 0.8164 Epoch 92/100 50000/50000 [==============================] - 127s 3ms/step - loss: 0.3699 - acc: 0.9950 - val_loss: 0.8140 - val_acc: 0.8226 Epoch 93/100 50000/50000 [==============================] - 131s 3ms/step - loss: 0.3694 - acc: 0.9950 - val_loss: 0.8342 - val_acc: 0.8210 Epoch 94/100 50000/50000 [==============================] - 134s 3ms/step - loss: 0.3698 - acc: 0.9946 - val_loss: 0.8938 - val_acc: 0.8019 Epoch 95/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3698 - acc: 0.9946 - val_loss: 0.8771 - val_acc: 0.8066 Epoch 96/100 50000/50000 [==============================] - 164s 3ms/step - loss: 0.3712 - acc: 0.9946 - val_loss: 0.8396 - val_acc: 0.8211 Epoch 97/100 50000/50000 [==============================] - 155s 3ms/step - loss: 0.3689 - acc: 0.9949 - val_loss: 0.8728 - val_acc: 0.8112 Epoch 98/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3663 - acc: 0.9953 - val_loss: 0.9615 - val_acc: 0.7902 Epoch 99/100 50000/50000 [==============================] - 133s 3ms/step - loss: 0.3714 - acc: 0.9944 - val_loss: 0.8414 - val_acc: 0.8188 Epoch 100/100 50000/50000 [==============================] - 138s 3ms/step - loss: 0.3682 - acc: 0.9956 - val_loss: 0.8055 - val_acc: 0.8266","title":"Model Training"},{"location":"examples/binarynet_cifar10/#model-output","text":"We can now plot a the final validation accuracy and loss plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0.9956000019311905 0.8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0.3663262344896793 0.7790719392895699","title":"Model Output"},{"location":"examples/mnist/","text":"Binarized Convolutional Neural Networks \u00b6 This tutorial demonstrates training a simple Binarized Convolutional Neural Network to classify MNIST digits. This simple network will achieve over 98% accuracy on the MNIST test set. Because this tutorial uses Larq and the Keras Sequential API , creating and training our model will take just a few lines of code. Import TensorFlow and Larq \u00b6 import tensorflow as tf import larq as lq Download and prepare the MNIST dataset \u00b6 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 Create the model \u00b6 This will create a simple binarized convolutional network. In the forward pass the quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (essentially the binarization is replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally the latent full precision weights are clipped to be between -1 and 1 using kernel_constraint=\"weight_clip\" . kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Here's the complete architecture of our model. Almost all parameter of the network a binarized (either -1 or 1). This will make the network extremly fast when deployed on a embedded device that supports binarized neural networks. lq . models . summary ( model ) Layer Outputs # 1-bit # 32-bit ------------------------ ---------------- --------- ---------- quant_conv2d_3 (-1, 26, 26, 32) 288 0 max_pooling2d_2 (-1, 13, 13, 32) 0 0 batch_normalization_v1_5 (-1, 13, 13, 32) 0 96 quant_conv2d_4 (-1, 11, 11, 64) 18432 0 max_pooling2d_3 (-1, 5, 5, 64) 0 0 batch_normalization_v1_6 (-1, 5, 5, 64) 0 192 quant_conv2d_5 (-1, 3, 3, 64) 36864 0 batch_normalization_v1_7 (-1, 3, 3, 64) 0 192 flatten_1 (-1, 576) 0 0 quant_dense_2 (-1, 64) 36864 0 batch_normalization_v1_8 (-1, 64) 0 192 quant_dense_3 (-1, 10) 640 0 batch_normalization_v1_9 (-1, 10) 0 30 activation_1 (-1, 10) 0 0 Total 93088 702 Total params: 93790 Trainable params: 93322 Non-trainable params: 468 Compile and train the model \u00b6 Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1/6 60000/60000 [==============================] - 72s 1ms/sample - loss: 0.6494 - acc: 0.9070\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2/6 60000/60000 [==============================] - 67s 1ms/sample - loss: 0.4760 - acc: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3/6 60000/60000 [==============================] - 67s 1ms/sample - loss: 0.4480 - acc: 0.9691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4365 - acc: 0.9718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4329 - acc: 0.9739\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4287 - acc: 0.9758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000/10000 [==============================] - 6s 576us/sample - loss: 0.4283 - acc: 0.9751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Evaluate the model \u00b6 print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97.51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Introduction to Larq"},{"location":"examples/mnist/#binarized-convolutional-neural-networks","text":"This tutorial demonstrates training a simple Binarized Convolutional Neural Network to classify MNIST digits. This simple network will achieve over 98% accuracy on the MNIST test set. Because this tutorial uses Larq and the Keras Sequential API , creating and training our model will take just a few lines of code.","title":"Binarized Convolutional Neural Networks"},{"location":"examples/mnist/#import-tensorflow-and-larq","text":"import tensorflow as tf import larq as lq","title":"Import TensorFlow and Larq"},{"location":"examples/mnist/#download-and-prepare-the-mnist-dataset","text":"( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1","title":"Download and prepare the MNIST dataset"},{"location":"examples/mnist/#create-the-model","text":"This will create a simple binarized convolutional network. In the forward pass the quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (essentially the binarization is replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally the latent full precision weights are clipped to be between -1 and 1 using kernel_constraint=\"weight_clip\" . kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Here's the complete architecture of our model. Almost all parameter of the network a binarized (either -1 or 1). This will make the network extremly fast when deployed on a embedded device that supports binarized neural networks. lq . models . summary ( model ) Layer Outputs # 1-bit # 32-bit ------------------------ ---------------- --------- ---------- quant_conv2d_3 (-1, 26, 26, 32) 288 0 max_pooling2d_2 (-1, 13, 13, 32) 0 0 batch_normalization_v1_5 (-1, 13, 13, 32) 0 96 quant_conv2d_4 (-1, 11, 11, 64) 18432 0 max_pooling2d_3 (-1, 5, 5, 64) 0 0 batch_normalization_v1_6 (-1, 5, 5, 64) 0 192 quant_conv2d_5 (-1, 3, 3, 64) 36864 0 batch_normalization_v1_7 (-1, 3, 3, 64) 0 192 flatten_1 (-1, 576) 0 0 quant_dense_2 (-1, 64) 36864 0 batch_normalization_v1_8 (-1, 64) 0 192 quant_dense_3 (-1, 10) 640 0 batch_normalization_v1_9 (-1, 10) 0 30 activation_1 (-1, 10) 0 0 Total 93088 702 Total params: 93790 Trainable params: 93322 Non-trainable params: 468","title":"Create the model"},{"location":"examples/mnist/#compile-and-train-the-model","text":"Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1/6 60000/60000 [==============================] - 72s 1ms/sample - loss: 0.6494 - acc: 0.9070\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2/6 60000/60000 [==============================] - 67s 1ms/sample - loss: 0.4760 - acc: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3/6 60000/60000 [==============================] - 67s 1ms/sample - loss: 0.4480 - acc: 0.9691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4365 - acc: 0.9718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4329 - acc: 0.9739\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6/6 60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4287 - acc: 0.9758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000/10000 [==============================] - 6s 576us/sample - loss: 0.4283 - acc: 0.9751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b","title":"Compile and train the model"},{"location":"examples/mnist/#evaluate-the-model","text":"print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97.51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Evaluate the model"}]}